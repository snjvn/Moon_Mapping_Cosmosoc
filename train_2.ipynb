{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a7c77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.19.5\n",
      "  Downloading numpy-1.19.5-cp39-cp39-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.4\n",
      "    Uninstalling numpy-1.23.4:\n",
      "      Successfully uninstalled numpy-1.23.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "matplotlib 3.7.1 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
      "scikit-image 0.21.0 requires numpy>=1.21.1, but you have numpy 1.19.5 which is incompatible.\n",
      "scipy 1.11.1 requires numpy<1.28.0,>=1.21.6, but you have numpy 1.19.5 which is incompatible.\n",
      "tensorflow 2.4.1 requires absl-py~=0.10, but you have absl-py 1.4.0 which is incompatible.\n",
      "tensorflow 2.4.1 requires flatbuffers~=1.12.0, but you have flatbuffers 2.0 which is incompatible.\n",
      "tensorflow 2.4.1 requires gast==0.3.3, but you have gast 0.4.0 which is incompatible.\n",
      "tensorflow 2.4.1 requires grpcio~=1.32.0, but you have grpcio 1.48.2 which is incompatible.\n",
      "tensorflow 2.4.1 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
      "tensorflow 2.4.1 requires tensorflow-estimator<2.5.0,>=2.4.0, but you have tensorflow-estimator 2.6.0 which is incompatible.\n",
      "tensorflow 2.4.1 requires termcolor~=1.1.0, but you have termcolor 2.1.0 which is incompatible.\n",
      "tensorflow 2.4.1 requires typing-extensions~=3.7.4, but you have typing-extensions 4.7.1 which is incompatible.\n",
      "tensorflow 2.4.1 requires wrapt~=1.12.1, but you have wrapt 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.19.5\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.19.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1535ce60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 22:56:15.986857: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "/home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.0)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85df5006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol\n",
      "data loaded\n",
      "data processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 22:57:55.471179: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-08-23 22:57:55.471681: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-08-23 22:57:55.473502: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination\n",
      "2023-08-23 22:57:55.473518: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ubuntu-Q470H6-M6\n",
      "2023-08-23 22:57:55.473522: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ubuntu-Q470H6-M6\n",
      "2023-08-23 22:57:55.473574: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 525.85.12\n",
      "2023-08-23 22:57:55.473586: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 390.157.0\n",
      "2023-08-23 22:57:55.473589: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 390.157.0 does not match DSO version 525.85.12 -- cannot find working devices in this configuration\n",
      "2023-08-23 22:57:55.473692: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-23 22:57:55.474982: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-08-23 22:57:55.903361: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1207959552 exceeds 10% of free system memory.\n",
      "2023-08-23 22:57:56.194396: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1207959552 exceeds 10% of free system memory.\n",
      "2023-08-23 22:57:56.275488: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1207959552 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 1 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 22:57:56.648514: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-08-23 22:57:56.651573: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3799900000 Hz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    /home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    /home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1434 predict_step\n        return self(x, training=False)\n    /home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/keras/engine/input_spec.py:271 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 0 is incompatible with layer model: expected shape=(None, 96, 96, 3), found shape=(None, 369, 65)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 248\u001b[0m\n\u001b[1;32m    245\u001b[0m             discriminator\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./output/dis_model\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m e)\n\u001b[1;32m    246\u001b[0m             gan\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./output/gan_model\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m e)\n\u001b[0;32m--> 248\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 219\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size)\u001b[0m\n\u001b[1;32m    217\u001b[0m image_batch_hr \u001b[38;5;241m=\u001b[39m x_train_hr[rand_nums]\n\u001b[1;32m    218\u001b[0m image_batch_lr \u001b[38;5;241m=\u001b[39m x_train_lr[rand_nums]\n\u001b[0;32m--> 219\u001b[0m generated_images_sr \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_batch_lr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m real_data_Y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(batch_size) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom_sample(batch_size)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.2\u001b[39m\n\u001b[1;32m    222\u001b[0m fake_data_Y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom_sample(batch_size)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.2\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1629\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1628\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m-> 1629\u001b[0m   tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1630\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1631\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name) \u001b[38;5;28;01mas\u001b[39;00m tm:\n\u001b[0;32m--> 828\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m   new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:871\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 871\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[1;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:725\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph \u001b[38;5;241m=\u001b[39m lifted_initializer_graph\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_deleter \u001b[38;5;241m=\u001b[39m FunctionDeleter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph)\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_stateful_fn \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 725\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_internal_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[1;32m    729\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2969\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2968\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m-> 2969\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2970\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3361\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3357\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[1;32m   3358\u001b[0m       args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[1;32m   3360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mmissed\u001b[38;5;241m.\u001b[39madd(call_context_key)\n\u001b[0;32m-> 3361\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mprimary[cache_key] \u001b[38;5;241m=\u001b[39m graph_function\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3196\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3191\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3192\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   3193\u001b[0m ]\n\u001b[1;32m   3194\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[1;32m   3195\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3199\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3204\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3205\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   3207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[1;32m   3208\u001b[0m     function_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[1;32m   3209\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m   3210\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m   3211\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m   3212\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:990\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    988\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m--> 990\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m    994\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[1;32m    995\u001b[0m                                   expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:634\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m     xla_context\u001b[38;5;241m.\u001b[39mExit()\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 634\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:977\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    976\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 977\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m    978\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    /home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    /home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1434 predict_step\n        return self(x, training=False)\n    /home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /home/ubuntu/anaconda3/envs/tf-new/lib/python3.9/site-packages/tensorflow/python/keras/engine/input_spec.py:271 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 0 is incompatible with layer model: expected shape=(None, 96, 96, 3), found shape=(None, 369, 65)\n"
     ]
    }
   ],
   "source": [
    "from Network import Generator, Discriminator\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "# import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Lambda, Input\n",
    "\n",
    "import skimage.transform\n",
    "from skimage import data, io, filters\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from skimage.transform import rescale, resize\n",
    "# from scipy.misc import imresize\n",
    "import os\n",
    "\n",
    "np.random.seed(10)\n",
    "image_shape = (384,384,3)\n",
    "\n",
    "def vgg_loss(y_true, y_pred):\n",
    "    \n",
    "    vgg19 = VGG19(include_top=False, weights='imagenet', input_shape=image_shape)\n",
    "    vgg19.trainable = False\n",
    "    for l in vgg19.layers:\n",
    "        l.trainable = False\n",
    "    loss_model = Model(inputs=vgg19.input, outputs=vgg19.get_layer('block5_conv4').output)\n",
    "    loss_model.trainable = False\n",
    "    return K.mean(K.square(loss_model(y_true) - loss_model(y_pred)))\n",
    "\n",
    "def get_gan_network(discriminator, shape, generator, optimizer):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=shape)\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = Model(inputs=gan_input, outputs=[x,gan_output])\n",
    "    gan.compile(loss=[vgg_loss, \"binary_crossentropy\"],\n",
    "                loss_weights=[1., 1e-3],\n",
    "                optimizer=optimizer)\n",
    "\n",
    "    return gan\n",
    "\n",
    "\n",
    "def load_path(path):\n",
    "    directories = []\n",
    "    if os.path.isdir(path):\n",
    "        directories.append(path)\n",
    "    for elem in os.listdir(path):\n",
    "        if os.path.isdir(os.path.join(path,elem)):\n",
    "            directories = directories + load_path(os.path.join(path,elem))\n",
    "            directories.append(os.path.join(path,elem))\n",
    "    return directories\n",
    "\n",
    "def normalize(input_data):\n",
    "\n",
    "    return ((input_data.astype(np.float32) - 127.5)/127.5)\n",
    "    \n",
    "def load_data_from_dirs(dirs, ext):\n",
    "    files = []\n",
    "    file_names = []\n",
    "    count = 0\n",
    "    for d in dirs:\n",
    "        for f in os.listdir(d): \n",
    "            if f.endswith(ext):\n",
    "                image = io.imread(os.path.join(d,f))\n",
    "#                 print(\"noice\",image[:,:,0])\n",
    "                if len(image.shape) > 2:\n",
    "#                     print(\"lol\")\n",
    "                    files.append(image[:,:,0])\n",
    "                    file_names.append(os.path.join(d,f))\n",
    "                else:\n",
    "                    files.append(image)\n",
    "                    file_names.append(os.path.join(d,f))\n",
    "                count = count + 1\n",
    "    return files     \n",
    "            \n",
    "# def load_data_from_dirs_resize(dirs, ext, size):\n",
    "#     files = []\n",
    "#     file_names = []\n",
    "#     count = 0\n",
    "#     for d in dirs:\n",
    "#         for f in os.listdir(d): \n",
    "#             if f.endswith(ext):\n",
    "#                 files.append(resize(io.imread(os.path.join(d,f)), size))\n",
    "#                 file_names.append(os.path.join(d,f))\n",
    "#                 count = count + 1\n",
    "#     return files     \n",
    "                        \n",
    "          \n",
    "def load_data(directory, ext):\n",
    "\n",
    "    files = load_data_from_dirs(load_path(directory), ext)\n",
    "    return files\n",
    "\n",
    "\n",
    "files = load_data(\"./resized_cropped_tmc/X_train\", \".png\")\n",
    "x_train_lr = files\n",
    "files = load_data(\"./resized_cropped_tmc/X_test\", \".png\")\n",
    "x_test_lr = files\n",
    "print(\"lol\")\n",
    "files = load_data(\"./resized_cropped_tmc/y_train\", \".png\")\n",
    "x_train_hr = files\n",
    "files = load_data(\"./ohrc_final_final/y_test\", \".png\")\n",
    "x_test_hr = files\n",
    "\n",
    "print(\"data loaded\")\n",
    "\n",
    "\n",
    "def hr_images(images):\n",
    "    images_hr = array(images)\n",
    "    return images_hr\n",
    "\n",
    "# def lr_images(images_real , downscale):\n",
    "    \n",
    "#     images = []\n",
    "#     for img in  range(len(images_real)):\n",
    "#         images.append(imresize(images_real[img], [images_real[img].shape[0]//downscale,images_real[img].shape[1]//downscale], interp='bicubic', mode=None))\n",
    "#     images_lr = array(images)\n",
    "#     return images_lr\n",
    "\n",
    "# def preprocess_HR(x):\n",
    "#     return np.divide(x.astype(np.float32), 127.5) - np.ones_like(x,dtype=np.float32)\n",
    "\n",
    "\n",
    "# def deprocess_HR(x):\n",
    "#     input_data = (input_data + 1) * 127.5\n",
    "#     return input_data.astype(np.uint8) \n",
    "\n",
    "\n",
    "# def preprocess_LR(x):\n",
    "#     return np.divide(x.astype(np.float32), 255.)\n",
    "\n",
    "\n",
    "# def deprocess_LR(x):\n",
    "#     x = np.clip(x*255, 0, 255)\n",
    "#     return x\n",
    "\n",
    "\n",
    "    \n",
    "def denormalize(input_data):\n",
    "    input_data = (input_data + 1) * 127.5\n",
    "    return input_data.astype(np.uint8) \n",
    "\n",
    "# def deprocess_LRS(x):\n",
    "#     x = np.clip(x*255, 0, 255)\n",
    "#     return x.astype(np.uint8)\n",
    "\n",
    "\n",
    "x_train_hr = hr_images(x_train_hr)\n",
    "\n",
    "x_train_lr = hr_images(x_train_lr)\n",
    "# x_train_lr = normalize(x_train_lr)\n",
    "\n",
    "\n",
    "x_test_hr = hr_images(x_test_hr)\n",
    "# x_test_hr = normalize(x_test_hr)\n",
    "\n",
    "x_test_lr = hr_images(x_test_lr)\n",
    "# x_test_lr = normalize(x_test_lr)\n",
    "\n",
    "print(\"data processed\")\n",
    "\n",
    "def plot_generated_images(epoch,generator, examples=3 , dim=(1, 3), figsize=(15, 5)):\n",
    "    \n",
    "    rand_nums = np.random.randint(0, x_test_hr.shape[0], size=examples)\n",
    "    image_batch_hr = x_test_hr[rand_nums]\n",
    "    image_batch_lr = x_test_lr[rand_nums]\n",
    "    gen_img = generator.predict(image_batch_lr)\n",
    "    generated_image = gen_img\n",
    "#     image_batch_lr = image_batch_lr\n",
    "    \n",
    "    #generated_image = deprocess_HR(generator.predict(image_batch_lr))\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    plt.subplot(dim[0], dim[1], 1)\n",
    "    plt.imshow(image_batch_lr[1], interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "        \n",
    "    plt.subplot(dim[0], dim[1], 2)\n",
    "    plt.imshow(generated_image[1], interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(dim[0], dim[1], 3)\n",
    "    plt.imshow(image_batch_hr[1], interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/gan_generated_image_epoch_%d.png' % epoch)\n",
    "    \n",
    "\n",
    "def train(epochs=1, batch_size=128):\n",
    "\n",
    "    downscale_factor = 4\n",
    "    \n",
    "    batch_count = int(x_train_hr.shape[0] / batch_size)\n",
    "    shape = (65)\n",
    "    \n",
    "    generator = Generator(shape).generator()\n",
    "    discriminator = Discriminator(image_shape).discriminator()\n",
    "\n",
    "    adam = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    generator.compile(loss=vgg_loss, optimizer=adam)\n",
    "    discriminator.compile(loss=\"binary_crossentropy\", optimizer=adam)\n",
    "    \n",
    "    shape = (image_shape[0]//downscale_factor, image_shape[1]//downscale_factor, 3)\n",
    "    gan = get_gan_network(discriminator, shape, generator, adam)\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        print ('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "        for _ in range(batch_count):\n",
    "            \n",
    "            rand_nums = np.random.randint(0, x_train_hr.shape[0], size=batch_size)\n",
    "            \n",
    "            image_batch_hr = x_train_hr[rand_nums]\n",
    "            image_batch_lr = x_train_lr[rand_nums]\n",
    "            generated_images_sr = generator.predict(image_batch_lr)\n",
    "\n",
    "            real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2\n",
    "            fake_data_Y = np.random.random_sample(batch_size)*0.2\n",
    "            \n",
    "            discriminator.trainable = True\n",
    "            \n",
    "            d_loss_real = discriminator.train_on_batch(image_batch_hr, real_data_Y)\n",
    "            d_loss_fake = discriminator.train_on_batch(generated_images_sr, fake_data_Y)\n",
    "            #d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "            \n",
    "            rand_nums = np.random.randint(0, x_train_hr.shape[0], size=batch_size)\n",
    "            image_batch_hr = x_train_hr[rand_nums]\n",
    "            image_batch_lr = x_train_lr[rand_nums]\n",
    "\n",
    "            gan_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2\n",
    "            discriminator.trainable = False\n",
    "            loss_gan = gan.train_on_batch(image_batch_lr, [image_batch_hr,gan_Y])\n",
    "            \n",
    "        print(\"Loss HR , Loss LR, Loss GAN\")\n",
    "        print(d_loss_real, d_loss_fake, loss_gan)\n",
    "\n",
    "        if e == 1 or e % 5 == 0:\n",
    "            plot_generated_images(e, generator)\n",
    "        if e % 300 == 0:\n",
    "            generator.save('./output/gen_model%d.h5' % e)\n",
    "            discriminator.save('./output/dis_model%d.h5' % e)\n",
    "            gan.save('./output/gan_model%d.h5' % e)\n",
    "\n",
    "train(20000,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2462fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
